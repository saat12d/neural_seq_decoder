2025-12-04 23:29:40,277 [INFO] Enabled TF32 for faster FP32 matmuls (Ampere+ GPUs)
2025-12-04 23:29:40,277 [INFO] ================================================================================
2025-12-04 23:29:40,277 [INFO] Starting training run
2025-12-04 23:29:40,277 [INFO] ================================================================================
2025-12-04 23:29:40,277 [INFO] Run Number: 14
2025-12-04 23:29:40,277 [INFO] Run Name: run14_onecycle_tuned
2025-12-04 23:29:40,277 [INFO] Run Purpose: Tuned OneCycleLR: max_lr=0.004, earlier peak (15%), tighter clipping (1.0), reduced noise, AMP+grad_accum (T4/Ampere optimized)
2025-12-04 23:29:40,278 [INFO] Output directory: /home/bciuser/projects/neural_seq_decoder/data/checkpoints/run14_onecycle_tuned
2025-12-04 23:29:40,278 [INFO] Dataset path: /home/bciuser/projects/neural_seq_decoder/data/formatted/ptDecoder_ctc
2025-12-04 23:29:40,278 [INFO] Batch size: 32
2025-12-04 23:29:40,278 [INFO] Total batches: 10000
2025-12-04 23:29:40,278 [INFO] Seed: 0
2025-12-04 23:29:42,404 [INFO] Dataset loaded: 24 training days
2025-12-04 23:29:42,405 [INFO] Training samples: 8800
2025-12-04 23:29:42,405 [INFO] Test samples: 880
2025-12-04 23:29:42,405 [INFO] ================================================================================
2025-12-04 23:29:42,405 [INFO] Model Architecture
2025-12-04 23:29:42,405 [INFO] ================================================================================
2025-12-04 23:29:42,405 [INFO] Input features: 256
2025-12-04 23:29:42,405 [INFO] Hidden units: 1024
2025-12-04 23:29:42,405 [INFO] GRU layers: 5
2025-12-04 23:29:42,405 [INFO] Output classes: 40 (+ 1 blank = 41)
2025-12-04 23:29:42,405 [INFO] Days (per-day embeddings): 24
2025-12-04 23:29:42,405 [INFO] Dropout: 0.4
2025-12-04 23:29:42,405 [INFO] Input dropout: 0.0
2025-12-04 23:29:42,405 [INFO] Layer norm: True
2025-12-04 23:29:42,405 [INFO] Bidirectional: True
2025-12-04 23:29:42,405 [INFO] Stride length: 4, Kernel length: 32
/opt/conda/lib/python3.10/site-packages/torch/functional.py:554: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:4314.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
2025-12-04 23:29:45,762 [INFO] Enabled cuDNN benchmark for faster training
2025-12-04 23:29:45,762 [INFO] torch.compile disabled (recommended off on T4 + GRU)
2025-12-04 23:29:45,763 [INFO] Total parameters: 135,424,553 (135.42M)
2025-12-04 23:29:45,763 [INFO] Trainable parameters: 135,424,553
2025-12-04 23:29:45,763 [INFO] ================================================================================
2025-12-04 23:29:45,763 [INFO] CTC Sanity Checks
2025-12-04 23:29:45,764 [INFO] ================================================================================
2025-12-04 23:29:45,764 [INFO] ✓ CTCLoss blank index: 0
2025-12-04 23:29:46,370 [INFO] ✓ T_eff calculation verified (min=31, max=150)
2025-12-04 23:29:46,388 [INFO] ✓ Labels verified: no blanks found in valid label spans (all labels >= 1)
2025-12-04 23:29:46,388 [INFO] ✓ Input lengths: min=633, max=633
2025-12-04 23:29:46,389 [INFO] ✓ Target lengths: min=11, max=58
2025-12-04 23:29:46,389 [INFO] ✓ T_eff lengths: min=31, max=150
2025-12-04 23:29:46,389 [INFO] ================================================================================
/home/bciuser/projects/neural_seq_decoder/src/neural_decoder/neural_decoder_trainer.py:346: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=True, dtype=torch.bfloat16):
2025-12-04 23:29:46,391 [INFO] Using mixed precision BF16 (Ampere+)
/home/bciuser/projects/neural_seq_decoder/src/neural_decoder/neural_decoder_trainer.py:359: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == torch.float16))
2025-12-04 23:29:46,392 [INFO]   Safety: log_softmax + CTCLoss computed in FP32
2025-12-04 23:29:47,561 [INFO] ================================================================================
2025-12-04 23:29:47,561 [INFO] Training Configuration
2025-12-04 23:29:47,562 [INFO] ================================================================================
2025-12-04 23:29:47,562 [INFO] Optimizer: ADAM
2025-12-04 23:29:47,562 [INFO] Peak LR: 0.004 (capped), End LR: 4e-06
2025-12-04 23:29:47,562 [INFO] Warmup steps: 0, Cosine steps: 10000
2025-12-04 23:29:47,562 [INFO] Weight decay: 1e-05
2025-12-04 23:29:47,562 [INFO] Gradient clipping: max_norm=1.0
2025-12-04 23:29:47,562 [INFO] Adaptive LR: DISABLED
2025-12-04 23:29:47,562 [INFO] Augmentation - White noise SD: 0.2
2025-12-04 23:29:47,562 [INFO] Augmentation - Constant offset SD: 0.05
2025-12-04 23:29:47,562 [INFO] Time masking - Prob: 0.1, Width: 40, Max masks: 2
2025-12-04 23:29:47,562 [INFO] Frequency masking - Prob: 0.1, Width: 12, Max masks: 2
2025-12-04 23:29:47,562 [INFO] CTC Decoding: Greedy
2025-12-04 23:29:47,562 [INFO] Using OneCycleLR scheduler:
2025-12-04 23:29:47,563 [INFO]   Max LR: 0.004, Start LR: 0.000160, End LR: 0.000004
2025-12-04 23:29:47,563 [INFO]   Warmup: 750 steps (15.0%)
2025-12-04 23:29:47,563 [INFO]   Early stop guard: reduce max_lr to 0.003 if PER > 0.38 at step 1200
2025-12-04 23:29:47,563 [INFO] ================================================================================
2025-12-04 23:29:47,563 [INFO] Starting training loop
2025-12-04 23:29:47,563 [INFO] ================================================================================
2025-12-04 23:29:47,563 [INFO] Gradient accumulation: 2 steps (effective batch size: 64)
/home/bciuser/projects/neural_seq_decoder/src/neural_decoder/neural_decoder_trainer.py:576: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=use_amp, dtype=dtype):
/home/bciuser/projects/neural_seq_decoder/src/neural_decoder/augmentations.py:91: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at /pytorch/aten/src/ATen/native/Convolution.cpp:1036.)
  return self.conv(input, weight=self.weight, groups=self.groups, padding="same")
/home/bciuser/projects/neural_seq_decoder/src/neural_decoder/neural_decoder_trainer.py:762: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=use_amp, dtype=dtype):
2025-12-04 23:29:51,448 [INFO] batch     0 | loss:  6.8381 (train:  3.5177) | per:  0.9067 (ma:  0.9067) | grad_norm: 1.2862 (max: 1.2862) | lr: 0.000160 | skipped: 0 | time:  0.04s | mem: 1.15GB/2.56GB
/home/bciuser/projects/neural_seq_decoder/src/neural_decoder/neural_decoder_trainer.py:923: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=use_amp, dtype=dtype):
2025-12-04 23:29:51,647 [INFO] Sample prediction (step 0):
2025-12-04 23:29:51,648 [INFO]   Target length: 20, Pred length: 14
2025-12-04 23:29:51,648 [INFO]   Target IDs (first 20): [32, 18, 1, 20, 28, 3, 29, 18, 40, 28, 18, 20, 3, 23, 29, 17, 9, 12, 9, 40]
2025-12-04 23:29:51,648 [INFO]   Pred IDs (first 20): [20, 9, 32, 2, 40, 10, 8, 22, 15, 16, 7, 14, 30, 11]
2025-12-04 23:29:51,648 [INFO]   Sample PER: 0.9000
2025-12-04 23:29:58,845 [INFO] ✓ New best checkpoint saved (PER: 0.9067, PER_MA: 0.9067)
2025-12-04 23:30:29,530 [INFO] batch   100 | loss:  2.5695 (train:  1.6439) | per:  0.7462 (ma:  0.8265) | grad_norm: 1.7511 (max: 5.4574) | lr: 0.000202 | skipped: 0 | time:  0.38s | mem: 2.21GB/6.02GB
2025-12-04 23:30:36,731 [INFO] ✓ New best checkpoint saved (PER: 0.7462, PER_MA: 0.8265)
2025-12-04 23:31:08,446 [INFO] batch   200 | loss:  1.7764 (train:  1.0833) | per:  0.5004 (ma:  0.7178) | grad_norm: 0.9475 (max: 1.4007) | lr: 0.000326 | skipped: 0 | time:  0.39s | mem: 2.22GB/6.03GB
2025-12-04 23:31:15,695 [INFO] ✓ New best checkpoint saved (PER: 0.5004, PER_MA: 0.7178)
2025-12-04 23:31:47,515 [INFO] batch   300 | loss:  1.4668 (train:  0.8146) | per:  0.4007 (ma:  0.6385) | grad_norm: 1.0731 (max: 1.5510) | lr: 0.000528 | skipped: 0 | time:  0.39s | mem: 2.22GB/6.03GB
2025-12-04 23:31:54,798 [INFO] ✓ New best checkpoint saved (PER: 0.4007, PER_MA: 0.6385)
2025-12-04 23:32:27,161 [INFO] batch   400 | loss:  1.3591 (train:  0.6834) | per:  0.3773 (ma:  0.5863) | grad_norm: 1.1431 (max: 1.6262) | lr: 0.000797 | skipped: 0 | time:  0.40s | mem: 2.22GB/6.03GB
2025-12-04 23:32:34,658 [INFO] ✓ New best checkpoint saved (PER: 0.3773, PER_MA: 0.5863)
2025-12-04 23:33:06,895 [INFO] batch   500 | loss:  1.2811 (train:  0.6563) | per:  0.3517 (ma:  0.5472) | grad_norm: 1.1867 (max: 1.7643) | lr: 0.001122 | skipped: 0 | time:  0.40s | mem: 2.21GB/6.03GB
2025-12-04 23:33:14,213 [INFO] ✓ New best checkpoint saved (PER: 0.3517, PER_MA: 0.5472)
2025-12-04 23:33:47,301 [INFO] batch   600 | loss:  1.2647 (train:  0.5953) | per:  0.3517 (ma:  0.5193) | grad_norm: 1.2004 (max: 1.7978) | lr: 0.001490 | skipped: 0 | time:  0.40s | mem: 2.22GB/6.03GB
2025-12-04 23:33:54,430 [INFO] ✓ New best checkpoint saved (PER: 0.3517, PER_MA: 0.5193)
2025-12-04 23:34:26,913 [INFO] batch   700 | loss:  1.3773 (train:  0.6056) | per:  0.3776 (ma:  0.5015) | grad_norm: 1.2412 (max: 1.6903) | lr: 0.001883 | skipped: 0 | time:  0.40s | mem: 2.21GB/6.03GB
2025-12-04 23:34:59,157 [INFO] batch   800 | loss:  1.3970 (train:  0.6429) | per:  0.3750 (ma:  0.4875) | grad_norm: 1.2913 (max: 1.8742) | lr: 0.002285 | skipped: 0 | time:  0.32s | mem: 2.22GB/6.03GB
2025-12-04 23:35:31,727 [INFO] batch   900 | loss:  1.4089 (train:  0.6202) | per:  0.3884 (ma:  0.4776) | grad_norm: 1.2654 (max: 1.7795) | lr: 0.002678 | skipped: 0 | time:  0.33s | mem: 2.21GB/6.03GB
2025-12-04 23:36:04,376 [INFO] batch  1000 | loss:  1.4969 (train:  0.6721) | per:  0.4042 (ma:  0.4709) | grad_norm: 1.3631 (max: 1.9319) | lr: 0.003045 | skipped: 0 | time:  0.33s | mem: 2.22GB/6.03GB
2025-12-04 23:36:04,593 [INFO] Sample prediction (step 1000):
2025-12-04 23:36:04,594 [INFO]   Target length: 20, Pred length: 15
2025-12-04 23:36:04,594 [INFO]   Target IDs (first 20): [32, 18, 1, 20, 28, 3, 29, 18, 40, 28, 18, 20, 3, 23, 29, 17, 9, 12, 9, 40]
2025-12-04 23:36:04,594 [INFO]   Pred IDs (first 20): [30, 18, 40, 1, 29, 40, 28, 17, 15, 11, 23, 9, 3, 9, 40]
2025-12-04 23:36:04,595 [INFO]   Sample PER: 0.6000
2025-12-04 23:36:37,082 [INFO] batch  1100 | loss:  1.5811 (train:  0.7156) | per:  0.4204 (ma:  0.4667) | grad_norm: 1.4127 (max: 1.9594) | lr: 0.003369 | skipped: 0 | time:  0.32s | mem: 2.22GB/6.03GB
2025-12-04 23:37:09,249 [WARNING] ⚠️  EARLY STOP GUARD TRIGGERED: PER 0.4291 > 0.38 at step 1200
2025-12-04 23:37:09,250 [WARNING]    Reducing OneCycleLR max_lr from 0.004 to 0.003
2025-12-04 23:37:09,251 [WARNING]    Scaled current LR: 0.003637 → 0.002728
2025-12-04 23:37:09,251 [INFO] batch  1200 | loss:  1.6113 (train:  0.7074) | per:  0.4291 (ma:  0.4638) | grad_norm: 1.4387 (max: 2.1150) | lr: 0.003637 | skipped: 0 | time:  0.32s | mem: 2.21GB/6.26GB
2025-12-04 23:37:41,474 [INFO] batch  1300 | loss:  1.6814 (train:  0.7820) | per:  0.4504 (ma:  0.4628) | grad_norm: 1.5366 (max: 2.1119) | lr: 0.003837 | skipped: 0 | time:  0.32s | mem: 2.22GB/6.26GB
2025-12-04 23:38:14,094 [INFO] batch  1400 | loss:  1.7476 (train:  0.8121) | per:  0.4722 (ma:  0.4635) | grad_norm: 1.5774 (max: 2.6308) | lr: 0.003960 | skipped: 0 | time:  0.33s | mem: 2.22GB/6.26GB
2025-12-04 23:38:45,915 [INFO] batch  1500 | loss:  1.7938 (train:  0.8388) | per:  0.4948 (ma:  0.4654) | grad_norm: 1.6657 (max: 2.2115) | lr: 0.004000 | skipped: 0 | time:  0.32s | mem: 2.21GB/6.26GB
2025-12-04 23:39:18,574 [INFO] batch  1600 | loss:  1.8452 (train:  0.8676) | per:  0.4907 (ma:  0.4669) | grad_norm: 1.7512 (max: 2.5777) | lr: 0.003999 | skipped: 0 | time:  0.33s | mem: 2.22GB/6.26GB
2025-12-04 23:39:51,251 [INFO] batch  1700 | loss:  1.8674 (train:  0.8831) | per:  0.4974 (ma:  0.4686) | grad_norm: 1.8265 (max: 3.1996) | lr: 0.003994 | skipped: 0 | time:  0.33s | mem: 2.22GB/6.26GB
2025-12-04 23:40:23,049 [INFO] batch  1800 | loss:  1.8961 (train:  0.8973) | per:  0.5130 (ma:  0.4709) | grad_norm: 1.8641 (max: 2.3167) | lr: 0.003988 | skipped: 0 | time:  0.32s | mem: 2.22GB/6.26GB
2025-12-04 23:40:55,350 [INFO] batch  1900 | loss:  1.9019 (train:  0.9255) | per:  0.5182 (ma:  0.4733) | grad_norm: 1.9522 (max: 2.7703) | lr: 0.003978 | skipped: 0 | time:  0.32s | mem: 2.22GB/6.26GB
2025-12-04 23:41:27,664 [INFO] batch  2000 | loss:  1.9624 (train:  0.9258) | per:  0.5334 (ma:  0.4762) | grad_norm: 2.0047 (max: 2.5146) | lr: 0.003966 | skipped: 0 | time:  0.32s | mem: 2.22GB/6.26GB
2025-12-04 23:41:27,818 [INFO] Sample prediction (step 2000):
2025-12-04 23:41:27,818 [INFO]   Target length: 20, Pred length: 7
2025-12-04 23:41:27,818 [INFO]   Target IDs (first 20): [32, 18, 1, 20, 28, 3, 29, 18, 40, 28, 18, 20, 3, 23, 29, 17, 9, 12, 9, 40]
2025-12-04 23:41:27,818 [INFO]   Pred IDs (first 20): [10, 13, 40, 20, 6, 6, 40]
2025-12-04 23:41:27,819 [INFO]   Sample PER: 0.8500
2025-12-04 23:42:00,537 [INFO] batch  2100 | loss:  1.9606 (train:  0.9476) | per:  0.5318 (ma:  0.4787) | grad_norm: 2.0457 (max: 2.7332) | lr: 0.003951 | skipped: 0 | time:  0.33s | mem: 2.22GB/6.26GB
2025-12-04 23:42:33,053 [INFO] batch  2200 | loss:  1.9690 (train:  0.9654) | per:  0.5340 (ma:  0.4811) | grad_norm: 2.1268 (max: 2.9077) | lr: 0.003933 | skipped: 0 | time:  0.33s | mem: 2.21GB/6.26GB
2025-12-04 23:43:05,519 [INFO] batch  2300 | loss:  1.9682 (train:  0.9479) | per:  0.5367 (ma:  0.4834) | grad_norm: 2.0998 (max: 2.7176) | lr: 0.003913 | skipped: 0 | time:  0.32s | mem: 2.21GB/6.26GB
2025-12-04 23:43:38,162 [INFO] batch  2400 | loss:  2.0499 (train:  0.9692) | per:  0.5410 (ma:  0.4857) | grad_norm: 2.1608 (max: 3.2961) | lr: 0.003890 | skipped: 0 | time:  0.33s | mem: 2.21GB/6.26GB
2025-12-04 23:44:10,231 [INFO] batch  2500 | loss:  2.0133 (train:  0.9799) | per:  0.5422 (ma:  0.4879) | grad_norm: 2.1584 (max: 2.7837) | lr: 0.003864 | skipped: 0 | time:  0.32s | mem: 2.21GB/6.26GB
2025-12-04 23:44:42,872 [INFO] batch  2600 | loss:  2.0024 (train:  0.9724) | per:  0.5404 (ma:  0.4898) | grad_norm: 2.1536 (max: 3.0526) | lr: 0.003836 | skipped: 0 | time:  0.33s | mem: 2.21GB/6.26GB
2025-12-04 23:45:15,003 [INFO] batch  2700 | loss:  1.9993 (train:  0.9726) | per:  0.5410 (ma:  0.4917) | grad_norm: 2.2912 (max: 4.1833) | lr: 0.003806 | skipped: 0 | time:  0.32s | mem: 2.22GB/6.26GB
2025-12-04 23:45:47,299 [INFO] batch  2800 | loss:  2.0633 (train:  0.9871) | per:  0.5533 (ma:  0.4938) | grad_norm: 2.2254 (max: 2.8953) | lr: 0.003773 | skipped: 0 | time:  0.32s | mem: 2.21GB/6.26GB
2025-12-04 23:46:20,437 [INFO] batch  2900 | loss:  1.9807 (train:  0.9831) | per:  0.5466 (ma:  0.4955) | grad_norm: 2.2299 (max: 2.9020) | lr: 0.003737 | skipped: 0 | time:  0.33s | mem: 2.21GB/6.26GB
2025-12-04 23:46:52,194 [INFO] batch  3000 | loss:  2.0207 (train:  0.9948) | per:  0.5463 (ma:  0.4972) | grad_norm: 2.1983 (max: 2.9250) | lr: 0.003700 | skipped: 0 | time:  0.32s | mem: 2.21GB/6.26GB
2025-12-04 23:46:52,409 [INFO] Sample prediction (step 3000):
2025-12-04 23:46:52,409 [INFO]   Target length: 20, Pred length: 13
2025-12-04 23:46:52,409 [INFO]   Target IDs (first 20): [32, 18, 1, 20, 28, 3, 29, 18, 40, 28, 18, 20, 3, 23, 29, 17, 9, 12, 9, 40]
2025-12-04 23:46:52,409 [INFO]   Pred IDs (first 20): [30, 17, 29, 13, 29, 31, 40, 11, 40, 9, 17, 26, 40]
2025-12-04 23:46:52,410 [INFO]   Sample PER: 0.8000
2025-12-04 23:47:24,745 [INFO] batch  3100 | loss:  1.9963 (train:  0.9758) | per:  0.5361 (ma:  0.4984) | grad_norm: 2.2300 (max: 3.2260) | lr: 0.003660 | skipped: 0 | time:  0.32s | mem: 2.21GB/6.26GB
2025-12-04 23:47:56,863 [INFO] batch  3200 | loss:  2.0235 (train:  0.9864) | per:  0.5516 (ma:  0.5000) | grad_norm: 2.2816 (max: 3.5663) | lr: 0.003617 | skipped: 0 | time:  0.32s | mem: 2.22GB/6.26GB
2025-12-04 23:48:29,160 [INFO] batch  3300 | loss:  1.9771 (train:  0.9909) | per:  0.5409 (ma:  0.5012) | grad_norm: 2.2001 (max: 3.6439) | lr: 0.003573 | skipped: 0 | time:  0.32s | mem: 2.21GB/6.26GB
2025-12-04 23:49:01,403 [INFO] batch  3400 | loss:  2.0011 (train:  0.9715) | per:  0.5521 (ma:  0.5027) | grad_norm: 2.1823 (max: 3.0654) | lr: 0.003526 | skipped: 0 | time:  0.32s | mem: 2.21GB/6.26GB
2025-12-04 23:49:33,845 [INFO] batch  3500 | loss:  2.0281 (train:  0.9917) | per:  0.5555 (ma:  0.5041) | grad_norm: 2.3174 (max: 5.9777) | lr: 0.003477 | skipped: 0 | time:  0.32s | mem: 2.21GB/6.26GB
2025-12-04 23:50:06,016 [INFO] batch  3600 | loss:  2.0817 (train:  0.9731) | per:  0.5623 (ma:  0.5057) | grad_norm: 2.2943 (max: 3.4082) | lr: 0.003426 | skipped: 0 | time:  0.32s | mem: 2.21GB/6.26GB
